# LLM Transcript Analyzer and RAG Summarizer
A FastAPI backend service for analyzing and querying transcripts using Retrieval-Augmented Generation (RAG) with support for both Firestore and local storage.

## Features

- Transcript upload and indexing with timestamp extraction
- Semantic Q&A with RAG using OpenAI or HuggingFace embeddings
- Query caching and history
- User access control and data isolation
- Support for both Firestore and local JSON storage
- Docker containerization

## Tech Stack

- **Backend**: FastAPI (Python)
- **Database**: Firestore (Google Cloud) or Local JSON
- **Embeddings**: OpenAI or HuggingFace Sentence Transformers
- **LLM**: Ollama (local) or OpenAI API
- **Vector Store**: ChromaDB
- **Containerization**: Docker


1. **Step 1 --- Set Up Firebase (Optional)**

**If using Firestore:**
- Create a Firebase project in the Firebase Console
- Enable Firestore Database
- Generate a service account key and download it as firebase-key.json
- Place the key file in the project root directory
- Update the FIRESTORE_PROJECT_ID in your .env file

2. **Step 2 --- Build and Run with Docker**
```bash
# Build the Docker image
docker build -t transcript-analyzer .

# Run the container
docker run -p 8000:8000 --env-file .env -v $(pwd)/data:/app/data transcript-analyzer
```

3. **Step 3 --- Access the Application**
- The API will be available at http://localhost:8000
- API Documentation: http://localhost:8000/docs
- Health Check: http://localhost:8000/health

4. **Step 4 ---  API Usage**

### Upload a Transcript
```bash
curl -X POST -F "user_id=user123" \
  -F "transcript_name=Startup Fundraising Tips" \
  -F "file=@path/to/transcript.txt" \
  http://localhost:8000/upload
```
Example transcript format:
```text
[00:00:00] Welcome to this talk on startup fundraising...
[00:01:30] One of the biggest mistakes founders make is focusing too much on the product...
[00:02:10] Founders often overlook market research...
```

### Query a Transcript
```bash
curl -X POST -H "Content-Type: application/json" \
  -d '{"user_id": "user123", "transcript_id": "transcript_id", "query": "What are the main points?"}' \
  http://localhost:8000/query
```

### Get User Transcripts
```bash
curl http://localhost:8000/transcripts/{user123}
```

5. **Step 5 --- Development**

### Without Docker
1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the application:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Project Structure
```text
transcript_analyzer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI application and routes
â”‚   â”œâ”€â”€ models.py        # Pydantic models
â”‚   â”œâ”€â”€ storage.py       # Firestore and local storage implementations
â”‚   â”œâ”€â”€ embeddings.py    # Embedding providers (OpenAI/HuggingFace)
â”‚   â”œâ”€â”€ rag.py          # RAG processing and query handling
â”‚   â”œâ”€â”€ utils.py        # Utility functions (transcript parsing)
â”‚   â”œâ”€â”€ firestore.py    # Firebase initialization
â”‚   â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ data/               # Data directory (mounted in Docker)
â”‚   â”œâ”€â”€ chroma/         # ChromaDB vector store
â”‚   â””â”€â”€ local_store.json # Local JSON database (if not using Firestore)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env               # Environment variables (create from .env.example)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Configuration Options

### Storage Backend
The application can use either Firestore or local JSON storage:
- Firestore: Set *FIRESTORE_PROJECT_ID* and provide *firebase-key.json*
- Local JSON: Don't set Firestore environment variables (data stored in *data/local_store.json*)

### Embedding Providers
- HuggingFace (default): Set *EMBEDDINGS_PROVIDER=huggingface*
- OpenAI: Set *EMBEDDINGS_PROVIDER=openai* and provide *OPENAI_API_KEY*

### LLM Providers
- Ollama (default): Set *LLM_PROVIDER=ollama*
- OpenAI: Set *LLM_PROVIDER=openai* and provide *OPENAI_API_KEY*

### Common Issues
- Firestore not connecting: Check your firebase-key.json and project ID
- CUDA errors: The application will automatically fall back to CPU mode
- Port already in use: Change the PORT in your .env file


## ðŸ“¦ API Overview
- `POST /upload_transcript` â€” form-data: `user_id`, file: `file` (.txt)
- `POST /query` â€” JSON: `{ user_id, transcript_id, question, top_k? }`
- `GET /transcripts` â€” query: `user_id`
- `GET /history` â€” query: `user_id`, `transcript_id`


### Input Transcript Format (example)
```
[00:00:00] Welcome to this talk on startup fundraising...
[00:01:30] One of the biggest mistakes founders make is focusing too much on the product...
[00:02:10] Founders often overlook market research...
```


## ðŸ”§ Switching Models
- Embeddings: set `EMBEDDINGS_PROVIDER=openai|huggingface`
- LLM: `LLM_PROVIDER=openai|ollama`
- `OLLAMA_MODEL=mistral` (example) and run `ollama serve`
- OpenAI: `OPENAI_API_KEY`, `OPENAI_MODEL=gpt-4o-mini` (example)